{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hug-0Okf8t"
      },
      "source": [
        "###<font color=\"black\"> ¬ª <b><font color=\"red\">Installing Dependencies </b>üíø</font> <font color=\"black\"> ¬´\n",
        "#####„Ö§Run this cell first before creating images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaGpmeILXSGl"
      },
      "outputs": [],
      "source": [
        "#@markdown <b>Run this first to install essential libraries!</b><br>\n",
        "#@markdown <small><p>Required to use the generator.\n",
        "from IPython.display import clear_output\n",
        "print(\"üì• | Connecting to Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "print(\"‚öôÔ∏è | Downloading libraries...\")\n",
        "!pip install diffusers\n",
        "!pip install torch==2.4.0 torchvision torchaudio\n",
        "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install opencv-python\n",
        "!pip install peft\n",
        "!pip install --upgrade huggingface_hub\n",
        "!pip install compel\n",
        "!pip install controlnet-aux\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "print(\"üìÅ | All essential libraries have been downloaded.\")\n",
        "print(\"üñå | You can start generating images now.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBZ305GvH7w"
      },
      "source": [
        "###<font color=\"black\"> ¬ª <b><font color=\"orange\">MultiControlNet<font color=\"black\">, <b><font color=\"magenta\"></b>IP-Adapter<font color=\"black\">, and <b><font color=\"Lime\">Inpainting</b> üîß</font> <font color=\"black\"> ¬´"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-sdjCI-xvy5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers import ControlNetModel, StableDiffusionXLPipeline, UniPCMultistepScheduler, StableDiffusionXLControlNetPipeline, AutoPipelineForInpainting\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline as pipe\n",
        "from transformers import CLIPVisionModelWithProjection\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Function to load the saved number from a file\n",
        "def load_number(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data['number']\n",
        "    except (FileNotFoundError, KeyError):\n",
        "        return None\n",
        "\n",
        "# Function to save the number to a file\n",
        "def save_number(filename, number):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump({'number': number}, file)\n",
        "def get_depth_map(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    detected_map = torch.from_numpy(image).float() / 255.0\n",
        "    depth_map = detected_map.permute(2, 0, 1)\n",
        "    return depth_map\n",
        "def get_depth_map_display(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    return image\n",
        "# Main function to handle the logic\n",
        "if __name__ == \"__main__\":\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    folder = \"/content/gdrive/MyDrive/\"\n",
        "    filename = os.path.join(folder, \"random_number.json\")\n",
        "    Freeze = False #@param {type:\"boolean\"}\n",
        "    saved_number = load_number(filename)\n",
        "    if not Freeze:\n",
        "        # Generate a new random number if Freeze is False\n",
        "        random_number = random.randint(1, 10000)\n",
        "        save_number(filename, random_number)\n",
        "        saved_number = load_number(filename)\n",
        "    else:\n",
        "        # Use the saved number if Freeze is True\n",
        "        if saved_number is not None:\n",
        "            saved_number = saved_number\n",
        "        else:\n",
        "            print(\"No saved number found. Please set Freeze to False to generate a new number first.\")\n",
        "    #login(\"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\")\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Prompt üñå</b><br>\n",
        "    #@markdown <small>What do you want to see from the image?</small><br>\n",
        "    #@markdown <small>Leave everything unchecked and set the IP-Adapter to \"None\" to generate using Text2Image pipeline.</small>\n",
        "    Prompt = \"ultra detailed, high resolution, best quality\" #@param {type:\"string\"}\n",
        "    Model = \"IDK-ab0ut/Yiffymix_v51-XL\" #@param {type:\"string\"}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Settings ‚öôÔ∏è</b><br>\n",
        "    #@markdown <small>Leave \"ip\" to use last pre-generated ControlNet image. Check the box and leave the link blank to use last pre-generated image. </small>\n",
        "    Width = 1024 #@param {type:\"slider\", min: 512, max:1536, step:64}\n",
        "    Height = 1024 #@param {type:\"slider\", min:512, max:1536, step:64}\n",
        "    Steps = 28 #@param {type:\"number\"}\n",
        "    Scale = 7.5 #@param {type:\"slider\", min:1, max:12, step:0.1}\n",
        "    #@markdown ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    minimum_canny_threshold = 25 #@param {type:\"slider\", min:10, max:500, step:5}\n",
        "    maximum_canny_threshold = 100 #@param {type:\"slider\", min:100, max:750, step:5}\n",
        "    Canny_Link = \"\" #@param {type:\"string\"}\n",
        "    Canny = False #@param {type:\"boolean\"}\n",
        "    Canny_Strength = 0.8 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    DepthMap_Link = \"\" #@param {type:\"string\"}\n",
        "    Depth_Map = False #@param {type:\"boolean\"}\n",
        "    Depth_Strength = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    OpenPose_Link = \"https://media.gettyimages.com/id/1355075031/photo/portrait-of-young-man-sitting-on-stool-in-studio.webp?s=612x612&w=gi&k=20&c=5bt45gsVrDCCwh4vyjBCMRQVBhNz-2GauVwuuSz5SJU=\" #@param {type:\"string\"}\n",
        "    Open_Pose = False #@param {type:\"boolean\"}\n",
        "    Open_Pose_Strength = 0.7 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    Inpainting_Image = \"pre-generated controlnet image\" #@param [\"pre-generated text2image image\", \"pre-generated controlnet image\", \"previous inpainting image\"]\n",
        "    Mask_Image = \"\" #@param {type:\"string\"}\n",
        "    Inpainting = False #@param {type:\"boolean\"}\n",
        "    Inpainting_Strength = 0.9 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
        "    #@markdown <small> You can use multiple direct links to the images using this format: </small>\n",
        "\n",
        "    #@markdown <small><small><small> https://example1.com/.../file.jpg, https://example2.com/.../file.jpg, ... </small></small></small>\n",
        "\n",
        "    #@markdown <small> Make sure to put a space between comma and the next link. </small>\n",
        "    IP_Adapter = \"ip-adapter_sdxl_vit-h.bin\" #@param [\"ip-adapter-plus_sdxl_vit-h.bin\", \"ip-adapter-plus-face_sdxl_vit-h.bin\", \"ip-adapter_sdxl_vit-h.bin\", \"None\"]\n",
        "    IP_Image_Link=\"https://upload.wikimedia.org/wikipedia/en/9/9e/Fox_Mccloud.png, https://static.wikia.nocookie.net/fantendo/images/a/a3/Fox_Star_Fox_2.png/revision/latest?cb=20171017122031, https://upload.wikimedia.org/wikipedia/en/9/9e/Fox_Mccloud.png, https://s1.zerochan.net/Fox.McCloud.600.323874.jpg\" #@param {type:\"string\"}\n",
        "    IP_Adapter_Strength= 1 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n",
        "    #@markdown ***\n",
        "    #@markdown <b>Image Generation Negative Prompt ‚õî</b><br>\n",
        "    #@markdown <small>What you <b>don't</b> want to see from the image? (optional)</small><br>\n",
        "    Negative_Prompt = \"(((watermark), bad anatomy, bad eyes, loli+)) malformed, deformed, disfigured, lowres \" #@param {type:\"string\"}\n",
        "    if Canny:\n",
        "        if Canny_Link == \"ip\":\n",
        "            Canny_link = \"/content/gdrive/MyDrive/gen_2.jpg\"\n",
        "        elif not Canny_Link:\n",
        "            Canny_link = \"/content/gdrive/MyDrive/gen.jpg\"\n",
        "        else:\n",
        "            Canny_link = Canny_Link\n",
        "    if Depth_Map:\n",
        "        if DepthMap_Link == \"ip\":\n",
        "            Depthmap_Link = \"/content/gdrive/MyDrive/gen_2.jpg\"\n",
        "        elif not DepthMap_Link:\n",
        "            Depthmap_Link = \"/content/gdrive/MyDrive/gen.jpg\"\n",
        "        else:\n",
        "            Depthmap_Link = DepthMap_Link\n",
        "    if Open_Pose:\n",
        "        if OpenPose_Link == \"ip\":\n",
        "            Openpose_Link = \"/content/gdrive/MyDrive/gen_2.jpg\"\n",
        "        elif not OpenPose_Link:\n",
        "            Openpose_Link = \"/content/gdrive/MyDrive/gen.jpg\"\n",
        "        else:\n",
        "            Openpose_Link = OpenPose_Link\n",
        "    if Inpainting:\n",
        "        if Canny or Depth_Map or Open_Pose:\n",
        "            raise TypeError(\"You checked both ControlNet and Inpainting, which will cause incompatibility issues during your run. As of now, there's no alternative way to merge StableDiffusionXLControlNetPipeline and StableDiffusionXLInpaintingPipeline without causing any issues. Perhaps you want to use only one of them?\")\n",
        "        if not Mask_Image:\n",
        "            raise ValueError(\"You checked Inpainting while you're leaving Mask_Image empty. Mask_Image is required for Inpainting!\")\n",
        "        else:\n",
        "            mask_image = load_image(Mask_Image).resize((1024, 1024))\n",
        "        if Inpainting_Image == \"pre-generated text2image image\":\n",
        "            inpaint_image = load_image(\"/content/gdrive/MyDrive/gen.jpg\").resize((1024, 1024))\n",
        "        elif Inpainting_Image == \"pre-generated controlnet image\":\n",
        "            inpaint_image = load_image(\"/content/gdrive/MyDrive/gen_2.jpg\").resize((1024, 1024))\n",
        "        elif Inpainting_Image == \"previous inpainting image\":\n",
        "            inpaint_image = load_image(\"/content/gdrive/MyDrive/gen_3.jpg\")\n",
        "        display(inpaint_image)\n",
        "        display(mask_image)\n",
        "    controlnets = []\n",
        "    images = []\n",
        "    controlnets_scale = []\n",
        "    if Canny:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"üèûÔ∏è | Converting image with Canny Edge Detection...\")\n",
        "        c_img = load_image(Canny_link)\n",
        "        image_canny = np.array(c_img)\n",
        "        image_canny = cv2.Canny(image_canny, minimum_canny_threshold, maximum_canny_threshold)\n",
        "        image_canny = image_canny[:, :, None]\n",
        "        image_canny = np.concatenate([image_canny, image_canny, image_canny], axis=2)\n",
        "        canny_image = Image.fromarray(image_canny)\n",
        "        print(\"‚úÖ | Canny Edge Detection is complete.\")\n",
        "        time.sleep(1)\n",
        "        display(canny_image.resize((1024, 1024)))\n",
        "        images.append(canny_image.resize((1024, 1024)))\n",
        "        controlnets_scale.append(Canny_Strength)\n",
        "    if Depth_Map:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\"))\n",
        "        print(\"üèûÔ∏è | Converting image with Depth Map...\")\n",
        "        image_depth = load_image(Depthmap_Link).resize((1024, 1024))\n",
        "        depth_estimator = pipe(\"depth-estimation\")\n",
        "        depth_map = get_depth_map(image_depth, depth_estimator).unsqueeze(0).half().to(\"cpu\")\n",
        "        images.append(depth_map)\n",
        "        depth_map_display = Image.fromarray(get_depth_map_display(image_depth, depth_estimator))\n",
        "        print(\"‚úÖ | Depth Map is complete.\")\n",
        "        controlnets_scale.append(Depth_Strength)\n",
        "        time.sleep(1)\n",
        "        display(depth_map_display)\n",
        "    if Open_Pose:\n",
        "        openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\").to(\"cpu\")\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16).to(\"cuda\"))\n",
        "        print(\"üèûÔ∏è | Converting image with Open Pose...\")\n",
        "        image_openpose = load_image(Openpose_Link)\n",
        "        openpose_image = openpose(image_openpose)\n",
        "        images.append(openpose_image.resize((1024, 1024)))\n",
        "        print(\"‚úÖ | Open Pose is done.\")\n",
        "        controlnets_scale.append(Open_Pose_Strength)\n",
        "        display(openpose_image.resize((1024, 1024)))\n",
        "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "        \"h94/IP-Adapter\",\n",
        "        subfolder=\"models/image_encoder\",\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "        pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "        pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    else:\n",
        "        pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    pipeline.enable_xformers_memory_efficient_attention()\n",
        "    generator = torch.Generator(\"cpu\").manual_seed(saved_number)\n",
        "    pipeline.safety_checker = None\n",
        "    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "    compel = Compel(tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True], truncate_long_prompts=False)\n",
        "    conditioning, pooled = compel([Prompt, Negative_Prompt])\n",
        "    torch.cuda.empty_cache()\n",
        "    if IP_Adapter != \"None\":\n",
        "        adapter_image = []\n",
        "\n",
        "        simple_Url = IP_Image_Link.split(\", \")\n",
        "        for link in simple_Url:\n",
        "            print(link)\n",
        "            adapter_image.append(load_image(link))\n",
        "        image_embeds = [adapter_image]\n",
        "        pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=IP_Adapter)\n",
        "        pipeline.set_ip_adapter_scale(IP_Adapter_Strength)\n",
        "\n",
        "    if not Canny and not Depth_Map and not Open_Pose and not Inpainting:\n",
        "        image_save = \"/content/gdrive/MyDrive/gen.jpg\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "        else:\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    elif Inpainting and not Canny and not Depth_Map and not Open_Pose:\n",
        "        image_save = \"/content/gdrive/MyDrive/gen_3.jpg\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, image=inpaint_image, mask_image=mask_image, generator=generator,strength=Inpainting_Strength).images[0]\n",
        "        else:\n",
        "            image = pipeline(prompt_embeds=conditioning[0:1], pooled_prompt_embeds=pooled[0:1],negative_prompt_embeds=conditioning[1:2], negative_pooled_prompt_embeds=pooled[1:2],num_inference_steps=Steps, ip_adapter_image=image_embeds, width=Width, height=Height, guidance_scale=Scale, clip_skip=2, generator=generator, image=inpaint_image, mask_image=mask_image, strength=Inpainting_Strength).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    else:\n",
        "        image_save = \"/content/gdrive/MyDrive/gen_2.jpg\"\n",
        "        if Inpainting:\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=2,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=2,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                ).images[0]\n",
        "        else:\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=2,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=2,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale\n",
        "                ).images[0]\n",
        "    print(image)\n",
        "    image.save(image_save)\n",
        "    display(image)\n",
        "    print(saved_number)\n",
        "    time.sleep(3)\n",
        "    os.kill(os.getpid(), 9)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "t8hug-0Okf8t"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}