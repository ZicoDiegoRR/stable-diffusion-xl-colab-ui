{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZicoDiegoRR/stable_diffusion_xl_colab_ui/blob/main/stable_diffusion_xl_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8hug-0Okf8t"
      },
      "source": [
        "###<font color=\"black\"> Â» <b><font color=\"red\">Installing Dependencies </b>ğŸ’¿</font> <font color=\"black\"> Â«\n",
        "#####ã…¤Run this cell first before creating images!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaGpmeILXSGl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown Run this first to install essential libraries!\n",
        "#@markdown Required to use the generator.\n",
        "from IPython.display import clear_output\n",
        "print(\"âš™ï¸ | Downloading libraries...\")\n",
        "!git clone https://github.com/ZicoDiegoRR/stable_diffusion_xl_colab_ui.git StableDiffusionXLColabUI\n",
        "!pip install -r StableDiffusionXLColabUI/requirements.txt\n",
        "!pip install -r StableDiffusionXLColabUI/requirements_torch.txt\n",
        "clear_output()\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
        "print(\"ğŸ“ | All essential libraries have been downloaded.\")\n",
        "print(\"ğŸ–Œ | You can start generating images now.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCBZ305GvH7w"
      },
      "source": [
        "###<font color=\"black\"> Â» <b><font color=\"orange\">MultiControlNet<font color=\"black\">, <b><font color=\"magenta\"></b>IP-Adapter<font color=\"black\">, and <b><font color=\"Lime\">Inpainting</b> ğŸ”§</font> <font color=\"black\"> Â«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-sdjCI-xvy5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from compel import Compel, ReturnedEmbeddingsType\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers import ControlNetModel, StableDiffusionXLPipeline, StableDiffusionXLControlNetPipeline, AutoPipelineForInpainting, AutoencoderKL\n",
        "from diffusers import DDPMScheduler, DPMSolverMultistepScheduler, DPMSolverSinglestepScheduler, KDPM2DiscreteScheduler, KDPM2AncestralDiscreteScheduler, EulerDiscreteScheduler, EulerAncestralDiscreteScheduler, HeunDiscreteScheduler, LMSDiscreteScheduler, DEISMultistepScheduler, UniPCMultistepScheduler, DDIMScheduler, PNDMScheduler\n",
        "from diffusers.utils import load_image, make_image_grid\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline as pipe\n",
        "from transformers import CLIPVisionModelWithProjection\n",
        "from google.colab import drive\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import re\n",
        "import os\n",
        "import subprocess\n",
        "import os.path\n",
        "import torch\n",
        "import random\n",
        "import json\n",
        "\n",
        "#@markdown <b>Run the cell to start!</b>\n",
        "\n",
        "#@markdown <small>Just run the cell and enjoy. (required to run the cell above first)</small>\n",
        "\n",
        "#@markdown <small>You can disable Google Drive by not permitting the notebook to access your Google Drive storage.</small>\n",
        "\n",
        "#@markdown <small>If the runtime got restarted, just run it again.</small>\n",
        "\n",
        "# Function to load parameters config\n",
        "def load_param(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            params = json.load(f)\n",
        "            print(f\"Found a config at {filename}.\")\n",
        "        return params\n",
        "    except FileNotFoundError:\n",
        "        return []\n",
        "\n",
        "# Function to save the data to a json\n",
        "def save_last(filename, data, type):\n",
        "    try:\n",
        "        if os.path.exists(filename):\n",
        "            with open(filename, 'r') as file:\n",
        "                existing_data = json.load(file)\n",
        "        else:\n",
        "            existing_data = {}\n",
        "\n",
        "        if type == \"[Text-to-Image]\":\n",
        "            existing_data['text2img'] = data\n",
        "        elif type == \"[ControlNet]\":\n",
        "            existing_data['controlnet'] = data\n",
        "        elif type == \"[Inpainting]\":\n",
        "            existing_data['inpaint'] = data\n",
        "        with open(filename, 'w') as file:\n",
        "            json.dump(existing_data, file, indent=4)\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "\n",
        "# Function to load last-generated image\n",
        "def load_last(filename, type):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data.get(type, None)\n",
        "    except (FileNotFoundError, json.JSONDecodeError):\n",
        "        return None\n",
        "\n",
        "# Function to load the saved data from a json\n",
        "def load_number(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            return data['saved']\n",
        "    except (FileNotFoundError, KeyError):\n",
        "        return None\n",
        "\n",
        "# Function to save the data to a json\n",
        "def save_number(filename, data):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump({'saved': data}, file)\n",
        "\n",
        "#Function to save parameters config (had to make separate JSON def to avoid confusion)\n",
        "def save_param(path, data):\n",
        "    with open(path, 'w') as file:\n",
        "        json.dump(data, file)\n",
        "\n",
        "# Function to convert image into depth map\n",
        "def get_depth_map(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    detected_map = torch.from_numpy(image).float() / 255.0\n",
        "    depth_map = detected_map.permute(2, 0, 1)\n",
        "    return depth_map\n",
        "\n",
        "# Only for display in output, nothing crazy\n",
        "def get_depth_map_display(image, depth_estimator):\n",
        "    image = depth_estimator(image)[\"depth\"]\n",
        "    image = np.array(image)\n",
        "    image = image[:, :, None]\n",
        "    image = np.concatenate([image, image, image], axis=2)\n",
        "    return image\n",
        "\n",
        "# Function to restart the runtime to free up some of the VRAM if there's a change in model or the pipeline\n",
        "def restart(new, old):\n",
        "    print(f\"New model is found. Your previous one ({old}) is different than your new one ({new}).\")\n",
        "    print(\"Restarting the runtime is necessary to load the new one.\")\n",
        "    time.sleep(2)\n",
        "    print(\"Restarting the runtime...\")\n",
        "    time.sleep(0.5)\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "# Loading the saved config for the IPyWidgets\n",
        "try:\n",
        "    drive.mount('/content/gdrive', force_remount=True)\n",
        "except Exception as e:\n",
        "    print(\"Excluding Google Drive storage...\")\n",
        "    time.sleep(1.5)\n",
        "Save_and_Connect_To_GDrive = True if os.path.exists(\"/content/gdrive/MyDrive\") else False\n",
        "config_path_drive = os.path.join(\"/content/gdrive/MyDrive\", \"parameters.json\")\n",
        "config_path = os.path.join(\"/content\", \"parameters.json\")\n",
        "cfg_ver = load_param(config_path_drive)\n",
        "cfg = load_param(config_path_drive) if cfg_ver else load_param(config_path)\n",
        "if not cfg:\n",
        "    print(\"No saved config found. Defaulting...\")\n",
        "\n",
        "# IPyWidgetsâ¬‡ï¸\n",
        "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "\n",
        "# Prompt Section\n",
        "prompt_widget = widgets.Text(value=cfg[0] if cfg else \"\", description=\"Prompt\", placeholder=\"Enter your prompt here\")\n",
        "model_widget = widgets.Text(value=cfg[1] if cfg else \"\", description=\"Model\", placeholder=\"HF's repository or direct URL\")\n",
        "model_format_widget = widgets.Dropdown(\n",
        "    options=[\"Pickle Tensor (.ckpt)\", \"Safe Tensor (.safetensors)\"],\n",
        "    value=cfg[2] if cfg else \"Safe Tensor (.safetensors)\",\n",
        "    description=\"Model Format\",\n",
        ")\n",
        "negative_prompt_widget = widgets.Text(value=cfg[3] if cfg else \"\", description=\"Negative Prompt\", placeholder=\"What you don't want to see?\")\n",
        "token_widget = widgets.Text(description=\"CivitAI Token\", placeholder=\"Avoid 401 error from CivitAI\")\n",
        "general_settings = widgets.VBox([widgets.HTML(value=\"<b>Image Generation PromptğŸ–Œï¸</b>\"),\n",
        "    prompt_widget,\n",
        "    model_widget,\n",
        "    model_format_widget,\n",
        "    negative_prompt_widget,\n",
        "    token_widget,\n",
        "    widgets.HTML(value=\"For safety reason, your token <b>won't be saved</b>.\")\n",
        "])\n",
        "\n",
        "# Image Generation Settings\n",
        "width_slider = widgets.IntSlider(min=512, max=1536, step=64, value=cfg[4] if cfg else 1024, description=\"Width\")\n",
        "height_slider = widgets.IntSlider(min=512, max=1536, step=64, value=cfg[5] if cfg else 1024, description=\"Height\")\n",
        "steps_slider = widgets.IntText(value=cfg[7] if cfg else 12, description=\"Steps\")\n",
        "scale_slider = widgets.FloatSlider(min=1, max=12, step=0.1, value=cfg[8] if cfg else 6, description=\"Scale\")\n",
        "vae_link_widget = widgets.Text(value=cfg[9] if cfg else \"\", description=\"VAE Link\", placeholder=\"VAE model link\")\n",
        "vae_config = widgets.Text(value=cfg[36] if cfg else \"\", description=\"VAE Config Link\", placeholder=\"VAE config link\")\n",
        "clip_skip_slider = widgets.IntSlider(min=0, max=12, step=1, value=cfg[10] if cfg else 2, description=\"Clip Skip\")\n",
        "image_settings = widgets.VBox([\n",
        "    width_slider,\n",
        "    height_slider,\n",
        "    steps_slider,\n",
        "    scale_slider,\n",
        "    vae_link_widget,\n",
        "    vae_config,\n",
        "    clip_skip_slider\n",
        "])\n",
        "\n",
        "# Scheduler Section\n",
        "scheduler_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        \"Default (defaulting to the model)\", \"DPM++ 2M\", \"DPM++ 2M SDE\",\n",
        "        \"DPM++ SDE\", \"DPM2\", \"DDPM\",\n",
        "        \"DPM2 a\", \"DDIM\", \"PNDM\", \"Euler\", \"Euler a\", \"Heun\", \"LMS\",\n",
        "        \"DEISMultistep\", \"UniPCMultistep\"\n",
        "    ],\n",
        "    value=cfg[6] if cfg else \"Default (defaulting to the model)\",\n",
        "    description=\"Scheduler\",\n",
        ")\n",
        "karras_bool = widgets.Checkbox(value=cfg[32] if cfg else False, description=\"Enable Karras\")\n",
        "vpred_bool = widgets.Checkbox(value=cfg[33] if cfg else False, description=\"Enable V-prediction\")\n",
        "sgmuniform_bool = widgets.Checkbox(value=cfg[34] if cfg else False, description=\"Enable SGMUniform\")\n",
        "res_betas_zero_snr = widgets.Checkbox(value=cfg[35] if cfg else False, description=\"Rescale beta zero SNR\")\n",
        "scheduler_settings = widgets.VBox([\n",
        "    scheduler_dropdown,\n",
        "    karras_bool,\n",
        "    vpred_bool,\n",
        "    sgmuniform_bool,\n",
        "    res_betas_zero_snr,\n",
        "    widgets.HTML(value=\"Rescaling the betas to have zero terminal SNR helps to achieve vibrant color, but not necessary.\")\n",
        "])\n",
        "\n",
        "# LoRA Section\n",
        "lora_urls_widget = widgets.Text(\n",
        "    value=cfg[11] if cfg else \"\",\n",
        "    description=\"LoRA URLs\",\n",
        "    placeholder=\"Enter LoRA URLs separated by commas\"\n",
        ")\n",
        "weight_scale_widget = widgets.Text(\n",
        "    value=cfg[12] if cfg else \"\",\n",
        "    description=\"Weights\",\n",
        "    placeholder=\"Enter weights separated by commas\"\n",
        ")\n",
        "lora_settings = widgets.VBox([\n",
        "    lora_urls_widget,\n",
        "    weight_scale_widget,\n",
        "])\n",
        "\n",
        "# ControlNet Section\n",
        "canny_min_slider = widgets.IntSlider(min=10, max=500, step=5, value=cfg[13] if cfg else 100, description=\"Min Threshold\")\n",
        "canny_max_slider = widgets.IntSlider(min=100, max=750, step=5, value=cfg[14] if cfg else 240, description=\"Max Threshold\")\n",
        "canny_link_widget = widgets.Text(value=cfg[15] if cfg else \"\", description=\"Canny Link\", placeholder=\"Image link\")\n",
        "canny_toggle = widgets.Checkbox(value=cfg[16] if cfg else False, description=\"Enable Canny\")\n",
        "canny_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[17] if cfg else 0.7, description=\"Canny Strength\")\n",
        "canny_settings = widgets.VBox([canny_min_slider, canny_max_slider, canny_link_widget, canny_toggle, canny_strength_slider])\n",
        "\n",
        "depth_map_link_widget = widgets.Text(value=cfg[18] if cfg else \"\", description=\"DepthMap Link\", placeholder=\"Image link\")\n",
        "depth_map_toggle = widgets.Checkbox(value=cfg[19] if cfg else False, description=\"Enable Depth Map\")\n",
        "depth_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[20] if cfg else 0.7, description=\"Depth Strength\")\n",
        "depth_settings = widgets.VBox([depth_map_link_widget, depth_map_toggle, depth_strength_slider])\n",
        "\n",
        "openpose_link_widget = widgets.Text(value=cfg[21] if cfg else \"\", description=\"OpenPose Link\", placeholder=\"Image link\")\n",
        "openpose_toggle = widgets.Checkbox(value=cfg[22] if cfg else False, description=\"Enable OpenPose\")\n",
        "openpose_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[23] if cfg else 0.7, description=\"OpenPose Strength\")\n",
        "openpose_settings = widgets.VBox([openpose_link_widget, openpose_toggle, openpose_strength_slider])\n",
        "\n",
        "controlnet_settings = widgets.Accordion([canny_settings, depth_settings, openpose_settings])\n",
        "controlnet_settings.set_title(0, \"CannyğŸ“\")\n",
        "controlnet_settings.set_title(1, \"Depth MapğŸ”ï¸\")\n",
        "controlnet_settings.set_title(2, \"Open PoseğŸ•ºğŸ»\")\n",
        "\n",
        "# Inpainting Section\n",
        "inpainting_image_dropdown = widgets.Combobox(\n",
        "    options=[\n",
        "        \"pre-generated text2image image\",\n",
        "        \"pre-generated controlnet image\",\n",
        "        \"previous inpainting image\"\n",
        "    ],\n",
        "    value=cfg[24] if cfg else \"pre-generated text2image image\",\n",
        "    description=\"Inpainting Image\",\n",
        "    ensure_option=False\n",
        ")\n",
        "mask_image_widget = widgets.Text(value=cfg[25] if cfg else \"\", description=\"Mask Image\", placeholder=\"Image link\")\n",
        "inpainting_toggle = widgets.Checkbox(value=cfg[26] if cfg else False, description=\"Enable Inpainting\")\n",
        "inpainting_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[27] if cfg else 0.9, description=\"Inpainting Strength\")\n",
        "inpainting_settings = widgets.VBox([\n",
        "    inpainting_image_dropdown,\n",
        "    mask_image_widget,\n",
        "    inpainting_toggle,\n",
        "    inpainting_strength_slider\n",
        "])\n",
        "\n",
        "# IP-Adapter Section\n",
        "ip_adapter_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        \"ip-adapter-plus_sdxl_vit-h.bin\",\n",
        "        \"ip-adapter-plus-face_sdxl_vit-h.bin\",\n",
        "        \"ip-adapter_sdxl_vit-h.bin\",\n",
        "        \"None\"\n",
        "    ],\n",
        "    value=cfg[28] if cfg else \"None\",\n",
        "    description=\"IP-Adapter\",\n",
        ")\n",
        "ip_image_link_widget = widgets.Text(value=cfg[29] if cfg else \"\", description=\"IP Image Link\", placeholder=\"Image links separated by commas\")\n",
        "ip_adapter_strength_slider = widgets.FloatSlider(min=0.1, max=1, step=0.1, value=cfg[30] if cfg else 0.8, description=\"Adapter Strength\")\n",
        "ip_settings = widgets.VBox([\n",
        "    ip_adapter_dropdown,\n",
        "    ip_image_link_widget,\n",
        "    ip_adapter_strength_slider\n",
        "])\n",
        "\n",
        "# Miscellaneous\n",
        "submit_button_widget = widgets.Button(disabled=False, button_style='', description=\"Generate\")\n",
        "dont_spam = widgets.HTML(value=\"Please <b>don't spam</b> the generate button!\")\n",
        "keep_generating = widgets.HTML(value=\"You still can generate even though the cell is complete executing.\")\n",
        "submit_display = widgets.VBox([submit_button_widget, dont_spam, keep_generating])\n",
        "\n",
        "freeze_widget = widgets.Checkbox(description=\"Use the same seed\", value=cfg[31] if cfg else False)\n",
        "misc_settings = widgets.VBox([freeze_widget])\n",
        "loaded_model = widgets.Text(value=\"\")\n",
        "loaded_pipeline = widgets.Text(value=\"\")\n",
        "\n",
        "# Accordion, Tab, and  UI display grouping\n",
        "advanced_settings_ui = widgets.Accordion([image_settings, scheduler_settings, lora_settings, controlnet_settings, inpainting_settings, ip_settings, misc_settings])\n",
        "advanced_settings_titles = [\"Image Settingsâš™ï¸\", \"Scheduler SettingsğŸ–¼ï¸âš™ï¸\", \"LoRA SettingsğŸ“ğŸ–Œï¸\", \"ControlNet Settings ğŸ–¼ï¸ğŸ”§\", \"Inpainting SettingsğŸ–¼ï¸ğŸ–Œï¸\", \"IP-Adapter Settings ğŸ–¼ï¸ğŸ“\", \"Miscellaneousâ¡ï¸ğŸ’½\"]\n",
        "for i, title in enumerate(advanced_settings_titles):\n",
        "    advanced_settings_ui.set_title(i, title)\n",
        "\n",
        "ui = widgets.Tab()\n",
        "ui.children = [general_settings, advanced_settings_ui]\n",
        "ui.set_title(0, \"General Settings\")\n",
        "ui.set_title(1, \"Advanced Settings\")\n",
        "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "\n",
        "# The IPyWidgets handler\n",
        "\n",
        "def widget():\n",
        "    clear_output()\n",
        "    submit_display.layout.display = \"inline-block\"\n",
        "    display(ui, submit_display)\n",
        "\n",
        "def submit(_):\n",
        "    submit_display.layout.display = \"none\"\n",
        "    submit_button()\n",
        "\n",
        "# Main logic\n",
        "def submit_button():\n",
        "    torch.backends.cudnn.benchmark=True\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:16\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    Freeze = freeze_widget.value\n",
        "\n",
        "    # Handling Google Drive and seed\n",
        "    folder = \"/content/gdrive/MyDrive/\"\n",
        "    filename = os.path.join(folder, \"random_number.json\")\n",
        "    saved_number = load_number(filename)\n",
        "    if not Freeze:\n",
        "        # Generate a new random number if Freeze is False\n",
        "        random_number = random.randint(1, 1000000000)\n",
        "        save_number(filename, random_number)\n",
        "        saved_number = load_number(filename)\n",
        "    else:\n",
        "        # Use the saved number if Freeze is True\n",
        "        if saved_number is not None:\n",
        "            saved_number = saved_number\n",
        "        else:\n",
        "            print(\"No saved seed found. Generating new one...\")\n",
        "            random_number = random.randint(1, 1000000000)\n",
        "            save_number(filename, random_number)\n",
        "            saved_number = load_number(filename)\n",
        "\n",
        "    # Handling user's inputâ¬‡ï¸\n",
        "    # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "    Prompt = prompt_widget.value\n",
        "    Model = model_widget.value\n",
        "    Model_Format = model_format_widget.value\n",
        "    Negative_Prompt = negative_prompt_widget.value\n",
        "\n",
        "    Width = width_slider.value\n",
        "    Height = height_slider.value\n",
        "    Steps = steps_slider.value\n",
        "    Scale = scale_slider.value\n",
        "    VAE_Link = vae_link_widget.value\n",
        "    VAE_Config = vae_config.value\n",
        "    Clip_Skip = clip_skip_slider.value\n",
        "\n",
        "    Scheduler = scheduler_dropdown.value\n",
        "    Karras = karras_bool.value\n",
        "    V_Prediction = vpred_bool.value\n",
        "    SGMUniform = sgmuniform_bool.value\n",
        "    Rescale_betas_to_zero_SNR = res_betas_zero_snr.value\n",
        "\n",
        "    LoRA_URLs = lora_urls_widget.value\n",
        "    Weight_Scale = weight_scale_widget.value\n",
        "    Token = token_widget.value\n",
        "\n",
        "    minimum_canny_threshold = canny_min_slider.value\n",
        "    maximum_canny_threshold = canny_max_slider.value\n",
        "    Canny_Link = canny_link_widget.value\n",
        "    Canny = canny_toggle.value\n",
        "    Canny_Strength = canny_strength_slider.value\n",
        "\n",
        "    DepthMap_Link = depth_map_link_widget.value\n",
        "    Depth_Map = depth_map_toggle.value\n",
        "    Depth_Strength = depth_strength_slider.value\n",
        "\n",
        "    OpenPose_Link = openpose_link_widget.value\n",
        "    Open_Pose = openpose_toggle.value\n",
        "    Open_Pose_Strength = openpose_strength_slider.value\n",
        "\n",
        "    Inpainting_Image = inpainting_image_dropdown.value\n",
        "    Mask_Image = mask_image_widget.value\n",
        "    Inpainting = inpainting_toggle.value\n",
        "    Inpainting_Strength = inpainting_strength_slider.value\n",
        "\n",
        "    IP_Adapter = ip_adapter_dropdown.value\n",
        "    IP_Image_Link = ip_image_link_widget.value\n",
        "    IP_Adapter_Strength = ip_adapter_strength_slider.value\n",
        "    # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "\n",
        "    # Selecting image\n",
        "    if Save_and_Connect_To_GDrive:\n",
        "        base_path = \"/content/gdrive/MyDrive\"\n",
        "    else:\n",
        "        base_path = \"/content\"\n",
        "    last_generation_loading = os.path.join(base_path, \"last_generation.json\")\n",
        "    if Canny:\n",
        "        if Canny_Link == \"inpaint\":\n",
        "            Canny_link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif Canny_Link == \"controlnet\":\n",
        "            Canny_link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not Canny_Link:\n",
        "            Canny_link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Canny_link = Canny_Link\n",
        "        if not Canny_link and not os.path.exists(Canny_link):\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "        else:\n",
        "            pipeline_type = \"controlnet\"\n",
        "    else:\n",
        "        Canny_link = \"\"\n",
        "    if Depth_Map:\n",
        "        if DepthMap_Link == \"inpaint\":\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif DepthMap_Link == \"controlnet\":\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not DepthMap_Link:\n",
        "            Depthmap_Link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Depthmap_Link = DepthMap_Link\n",
        "        if not Depthmap_Link and not os.path.exists(Depthmap_Link):\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "        else:\n",
        "            pipeline_type = \"controlnet\"\n",
        "    else:\n",
        "        Depthmap_Link = \"\"\n",
        "    if Open_Pose:\n",
        "        if OpenPose_Link == \"inpaint\":\n",
        "            Openpose_Link = load_last(last_generation_loading, 'inpaint')\n",
        "        elif OpenPose_Link == \"controlnet\":\n",
        "            Openpose_Link = load_last(last_generation_loading, 'controlnet')\n",
        "        elif not OpenPose_Link:\n",
        "            Openpose_Link = load_last(last_generation_loading, 'text2img')\n",
        "        else:\n",
        "            Openpose_Link = OpenPose_Link\n",
        "        if Openpose_Link is None and not os.path.exists(Openpose_Link):\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "        else:\n",
        "            pipeline_type = \"controlnet\"\n",
        "    else:\n",
        "        Openpose_Link = \"\"\n",
        "    active_inpaint = False\n",
        "    if Inpainting:\n",
        "        if Canny or Depth_Map or Open_Pose:\n",
        "            raise TypeError(\"You checked both ControlNet and Inpainting, which will cause incompatibility issues during your run. As of now, there's no alternative way to merge StableDiffusionXLControlNetPipeline and StableDiffusionXLInpaintingPipeline without causing any issues. Perhaps you want to use only one of them?\")\n",
        "        if not Mask_Image:\n",
        "            raise ValueError(\"You checked Inpainting while you're leaving Mask_Image empty. Mask_Image is required for Inpainting!\")\n",
        "        if Inpainting_Image == \"pre-generated text2image image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'text2img')\n",
        "        elif Inpainting_Image == \"pre-generated controlnet image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'controlnet')\n",
        "        elif Inpainting_Image == \"previous inpainting image\":\n",
        "            inpaint_img = load_last(last_generation_loading, 'inpaint')\n",
        "        else:\n",
        "            inpaint_image = Inpainting_Image\n",
        "        if inpaint_img is not None and os.path.exists(inpaint_img):\n",
        "            pipeline_type = \"inpaint\"\n",
        "            inpaint_image = load_image(inpaint_img).resize((1024, 1024))\n",
        "            mask_image = load_image(Mask_Image).resize((1024, 1024))\n",
        "            active_inpaint = True\n",
        "            display(make_image_grid([inpaint_image, mask_image], rows=1, cols=2))\n",
        "        else:\n",
        "            print(\"No generated image found. Defaulting to Text-to-Image...\")\n",
        "    if not IP_Image_Link and IP_Adapter != \"None\":\n",
        "        raise ValueError(f\"You selected {IP_Adapter}, but left the IP_Image_Link empty. Please change the IP_Adapter to None or add at least one image in IP_Image_Link!\")\n",
        "    if not Canny_link and not Depthmap_Link and not Openpose_Link and not active_inpaint:\n",
        "        pipeline_type = \"text2img\"\n",
        "\n",
        "    # Saving parameters config 1st phase\n",
        "    params = [\n",
        "        prompt_widget.value,\n",
        "        model_widget.value,\n",
        "        model_format_widget.value,\n",
        "        negative_prompt_widget.value,\n",
        "        width_slider.value,\n",
        "        height_slider.value,\n",
        "        scheduler_dropdown.value,\n",
        "        steps_slider.value,\n",
        "        scale_slider.value,\n",
        "        vae_link_widget.value,\n",
        "        clip_skip_slider.value,\n",
        "        lora_urls_widget.value,\n",
        "        weight_scale_widget.value,\n",
        "        canny_min_slider.value,\n",
        "        canny_max_slider.value,\n",
        "        canny_link_widget.value,\n",
        "        canny_toggle.value,\n",
        "        canny_strength_slider.value,\n",
        "        depth_map_link_widget.value,\n",
        "        depth_map_toggle.value,\n",
        "        depth_strength_slider.value,\n",
        "        openpose_link_widget.value,\n",
        "        openpose_toggle.value,\n",
        "        openpose_strength_slider.value,\n",
        "        inpainting_image_dropdown.value,\n",
        "        mask_image_widget.value,\n",
        "        inpainting_toggle.value,\n",
        "        inpainting_strength_slider.value,\n",
        "        ip_adapter_dropdown.value,\n",
        "        ip_image_link_widget.value,\n",
        "        ip_adapter_strength_slider.value,\n",
        "        freeze_widget.value,\n",
        "        karras_bool.value,\n",
        "        vpred_bool.value,\n",
        "        sgmuniform_bool.value,\n",
        "        res_betas_zero_snr.value,\n",
        "        vae_config.value\n",
        "    ]\n",
        "    save_param(f\"{base_path}/parameters.json\", params)\n",
        "\n",
        "    # Checking if previous loaded model or pipeline is the same as the new one\n",
        "    if loaded_model.value and loaded_model.value != model_widget.value:\n",
        "        restart(model_widget.value, loaded_model.value)\n",
        "    if loaded_pipeline.value and loaded_pipeline.value != pipeline_type:\n",
        "        restart(pipeline_type, loaded_pipeline.value)\n",
        "\n",
        "    # Logic to handle ControlNet and/or MultiControlNets\n",
        "    controlnets = []\n",
        "    images = []\n",
        "    controlnets_scale = []\n",
        "    if Canny and Canny_link is not None and os.path.exists(Canny_link):\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-canny-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True, low_cpu_mem_usage=True).to(\"cuda\"))\n",
        "        print(\"ğŸï¸ | Converting image with Canny Edge Detection...\")\n",
        "        c_img = load_image(Canny_link)\n",
        "        image_canny = np.array(c_img)\n",
        "        image_canny = cv2.Canny(image_canny, minimum_canny_threshold, maximum_canny_threshold)\n",
        "        image_canny = image_canny[:, :, None]\n",
        "        image_canny = np.concatenate([image_canny, image_canny, image_canny], axis=2)\n",
        "        canny_image = Image.fromarray(image_canny)\n",
        "        print(\"âœ… | Canny Edge Detection is complete.\")\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([c_img, canny_image.resize((1024, 1024))], rows=1, cols=2))\n",
        "        images.append(canny_image.resize((1024, 1024)))\n",
        "        controlnets_scale.append(Canny_Strength)\n",
        "    if Depth_Map and Depthmap_Link is not None:\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"diffusers/controlnet-depth-sdxl-1.0\", torch_dtype=torch.float16, use_safetensors=True, low_cpu_mem_usage=True).to(\"cuda\"))\n",
        "        print(\"ğŸï¸ | Converting image with Depth Map...\")\n",
        "        image_depth = load_image(Depthmap_Link).resize((1024, 1024))\n",
        "        depth_estimator = pipe(\"depth-estimation\")\n",
        "        depth_map = get_depth_map(image_depth, depth_estimator).unsqueeze(0).half().to(\"cpu\")\n",
        "        images.append(depth_map)\n",
        "        depth_map_display = Image.fromarray(get_depth_map_display(image_depth, depth_estimator))\n",
        "        print(\"âœ… | Depth Map is complete.\")\n",
        "        controlnets_scale.append(Depth_Strength)\n",
        "        time.sleep(1)\n",
        "        display(make_image_grid([image_depth, depth_map_display], rows=1, cols=2))\n",
        "    if Open_Pose and Openpose_Link is not None:\n",
        "        openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\").to(\"cpu\")\n",
        "        controlnets.append(ControlNetModel.from_pretrained(\"thibaud/controlnet-openpose-sdxl-1.0\", torch_dtype=torch.float16, low_cpu_mem_usage=True).to(\"cuda\"))\n",
        "        print(\"ğŸï¸ | Converting image with Open Pose...\")\n",
        "        image_openpose = load_image(Openpose_Link)\n",
        "        openpose_image = openpose(image_openpose)\n",
        "        images.append(openpose_image.resize((1024, 1024)))\n",
        "        print(\"âœ… | Open Pose is done.\")\n",
        "        controlnets_scale.append(Open_Pose_Strength)\n",
        "        display(make_image_grid([image_openpose, openpose_image.resize((1024, 1024))], rows=1, cols=2))\n",
        "\n",
        "    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
        "        \"h94/IP-Adapter\",\n",
        "        subfolder=\"models/image_encoder\",\n",
        "        torch_dtype=torch.float16,\n",
        "    ) if IP_Adapter != \"None\" else None\n",
        "\n",
        "    # Logic to handle VAE\n",
        "    if VAE_Link and VAE_Config:\n",
        "        if not os.path.exists(\"/content/VAE\"):\n",
        "            os.mkdir(\"VAE\")\n",
        "        vae_filename = VAE_Link.replace(\"/\", \"_\").replace(\".\", \"_\") + \".safetensors\"\n",
        "        if VAE_Link.startswith(\"http\"):\n",
        "            if \"civitai.com\" in VAE_Link:\n",
        "                if \"?\" in VAE_Link or \"&\" in VAE_Link:\n",
        "                    vae_link = VAE_Link + \"&token=\" + Token\n",
        "                else:\n",
        "                    vae_link = VAE_Link + \"token=\" + Token\n",
        "            else:\n",
        "                vae_link = VAE_Link\n",
        "            if not os.path.exists(f\"/content/VAE/{vae_filename}\"):\n",
        "                !cd /content/VAE; wget -O \"$vae_filename\" \"$vae_link\"\n",
        "                !cd /content/VAE; wget -N \"$VAE_Config\"\n",
        "        vae_path = f\"/content/VAE/{vae_filename}\" if VAE_Link.startswith(\"http\") else VAE_Link\n",
        "        vae = AutoencoderKL.from_single_file(vae_path, config=\"/content/VAE/config.json\", torch_dtype=torch.float16, local_files_only=True)\n",
        "    elif VAE_Link and not VAE_Config:\n",
        "        vae = None\n",
        "        print(\"You inputted a VAE link, but not the config. Config is essential to load the model.\")\n",
        "        print(\"Skipping VAE...\")\n",
        "\n",
        "    # Logic to differentiate if the model is Hugging Face's repository\n",
        "    global pipeline\n",
        "    if Model.count(\"/\") == 1:\n",
        "        if not controlnets and not active_inpaint and (pipeline_type != \"text2img\" or not loaded_pipeline.value) and (not loaded_model.value or model_widget.value != loaded_model.value):\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLPipeline.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif active_inpaint and not controlnets and (pipeline_type != \"inpaint\" or not loaded_pipeline.value) and (not loaded_model.value or model_widget.value != loaded_model.value):\n",
        "            if VAE_Link:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = AutoPipelineForInpainting.from_pretrained(Model, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        elif (pipeline_type != \"controlnet\" or not loaded_pipeline.value) and (not loaded_model.value or model_widget.value != loaded_model.value):\n",
        "            if VAE_Link:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            else:\n",
        "                pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(Model, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "    else:\n",
        "        if not os.path.exists(\"/content/Checkpoint\"):\n",
        "            os.mkdir(\"Checkpoint\")\n",
        "        if \".ckpt\" in Model_Format:\n",
        "            format = \".ckpt\"\n",
        "        elif \".safetensors\" in Model_Format:\n",
        "            format = \".safetensors\"\n",
        "        checkpoint_name = f\"checkpoint_model{format}\"\n",
        "        if Token and \"civitai.com\" in Model:\n",
        "            if \"?\" in Model or \"&\" in Model:\n",
        "                checkpoint_link = f\"{Model}&token={Token}\"\n",
        "            else:\n",
        "                checkpoint_link = f\"{Model}token={Token}\"\n",
        "        else:\n",
        "            checkpoint_link = Model\n",
        "        Model_folder = Model.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "        Model_path = f\"/content/Checkpoint/{Model_folder}/{checkpoint_name}\"\n",
        "        Model_path_folder = f\"/content/Checkpoint/{Model_folder}\"\n",
        "        if not os.path.exists(Model_path):\n",
        "            if not os.path.exists(Model_path_folder):\n",
        "                os.mkdir(Model_path_folder)\n",
        "            !cd \"$Model_path_folder\"; wget -O \"$checkpoint_name\" \"$checkpoint_link\"\n",
        "        try:\n",
        "            if not controlnets and not active_inpaint and (pipeline_type != \"text2img\" or not loaded_pipeline.value) and (not loaded_model.value or model_widget.value != loaded_model.value):\n",
        "                if VAE_Link:\n",
        "                    pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                else:\n",
        "                    pipeline = StableDiffusionXLPipeline.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            elif active_inpaint and not controlnets and (pipeline_type != \"inpaint\" or not loaded_pipeline.value) and (not loaded_model.value or model_widget.value != loaded_model.value):\n",
        "                if VAE_Link:\n",
        "                    pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                else:\n",
        "                    pipeline = AutoPipelineForInpainting.from_single_file(Model_path, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
        "            elif (pipeline_type != \"controlnet\" or not loaded_pipeline.value) and (not loaded_model.value or model_widget.value != loaded_model.value):\n",
        "                if VAE_Link:\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, vae=vae, torch_dtype=torch.float16).to(\"cuda\")\n",
        "                else:\n",
        "                    pipeline = StableDiffusionXLControlNetPipeline.from_single_file(Model_path, image_encoder=image_encoder, controlnet=controlnets, torch_dtype=torch.float16).to(\"cuda\")\n",
        "        except (ValueError, OSError):\n",
        "            pass\n",
        "            os.remove(Model_path)\n",
        "            if not Token and \"civitai.com\" in Model:\n",
        "                Warning = \"You inputted a CivitAI's link, but your token is empty. It's possible that you got unauthorized access during the download.\"\n",
        "            else:\n",
        "                Warning = \"Did you input the correct link? Or did you use the correct format?\"\n",
        "            raise TypeError(f\"The link ({Model}) contains unsupported file or the download was corrupted. {Warning}\")\n",
        "\n",
        "    if not loaded_model.value:\n",
        "        loaded_model.value = model_widget.value\n",
        "    if not loaded_pipeline.value:\n",
        "        loaded_pipeline.value = pipeline_type\n",
        "\n",
        "    # Seed, safety checker, and memory attention (Xformers)\n",
        "    pipeline.enable_xformers_memory_efficient_attention()\n",
        "    generator = torch.Generator(\"cpu\").manual_seed(saved_number)\n",
        "    pipeline.safety_checker = None\n",
        "\n",
        "    # Handling schedulers\n",
        "    Prediction_type = \"v_prediction\" if V_Prediction else \"epsilon\"\n",
        "    scheduler_args = {\"prediction_type\": Prediction_type,\n",
        "                           \"use_karras_sigmas\": Karras,\n",
        "                           \"rescale_betas_zero_snr\": Rescale_betas_to_zero_SNR\n",
        "                           }\n",
        "    if SGMUniform:\n",
        "      scheduler_args[\"timestep_spacing\"] = \"trailing\"\n",
        "    Scheduler_used = [\"\", f\"{Scheduler} \", \"\", \"\", \"\"]\n",
        "    Scheduler_used[0] = \"V-Prediction \" if Prediction_type == \"v_prediction\" else \"\"\n",
        "    Scheduler_used[2] = \"Karras \" if Karras else \"\"\n",
        "    Scheduler_used[3] = \"SGMUniform \" if SGMUniform else \"\"\n",
        "    Scheduler_used[4] = \"with zero SNR betas rescaling\" if Rescale_betas_to_zero_SNR else \"\"\n",
        "    if Scheduler == \"DPM++ 2M\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DPM++ 2M SDE\":\n",
        "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, algorithm_type=\"sde-dpmsolver++\", **scheduler_args)\n",
        "    elif Scheduler == \"DPM++ SDE\":\n",
        "        pipeline.scheduler = DPMSolverSinglestepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DPM2\":\n",
        "        pipeline.scheduler = KDPM2DiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DPM2 a\":\n",
        "        pipeline.scheduler = KDPM2AncestralDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DDPM\":\n",
        "        pipeline.scheduler = DDPMScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"Euler\":\n",
        "        pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"Euler a\":\n",
        "        pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"Heun\":\n",
        "        pipeline.scheduler = HeunDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"LMS\":\n",
        "        pipeline.scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DEISMultistep\":\n",
        "        pipeline.scheduler = DEISMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"UniPCMultistep\":\n",
        "        pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"DDIM\":\n",
        "        pipeline.scheduler = DDIMScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "    elif Scheduler == \"PNDM\":\n",
        "        pipeline.scheduler = PNDMScheduler.from_config(pipeline.scheduler.config, **scheduler_args)\n",
        "\n",
        "    # Prompt weighting using Compel\n",
        "    compel = Compel(tokenizer=[pipeline.tokenizer, pipeline.tokenizer_2], text_encoder=[pipeline.text_encoder, pipeline.text_encoder_2], returned_embeddings_type=ReturnedEmbeddingsType.PENULTIMATE_HIDDEN_STATES_NON_NORMALIZED, requires_pooled=[False, True], truncate_long_prompts=False)\n",
        "    conditioning, pooled = compel([Prompt, Negative_Prompt])\n",
        "\n",
        "    # Logic to load LoRA(s)\n",
        "    if LoRA_URLs:\n",
        "        lora_list = []\n",
        "        lora_path = []\n",
        "        lora_links = [word for word in re.split(r\"\\s*,\\s*\", LoRA_URLs) if word]\n",
        "        if not os.path.exists(\"/content/LoRAs\"):\n",
        "            os.mkdir(\"LoRAs\")\n",
        "        if not Weight_Scale:\n",
        "            scales_string = [\"1\"] * len(lora_links)\n",
        "        elif Weight_Scale and len(re.split(r\",| ,\", Weight_Scale)) < len(lora_links):\n",
        "            scales_string = re.split(r\",| ,\", Weight_Scale)\n",
        "            for j in range(len(lora_links) - len(scales_string)):\n",
        "                scales_string.append(\"1\")\n",
        "        else:\n",
        "            scales_string = re.split(r\"\\s*,\\s*\", Weight_Scale)\n",
        "        scales = [float(num) for num in scales_string if num]\n",
        "\n",
        "        for i, link in enumerate(lora_links, start=1):\n",
        "            path_file = os.path.join(\"/content/LoRAs\", link.replace(\"/\", \"_\").replace(\".\", \"_\"))\n",
        "            if not os.path.exists(path_file) and \"http\" in link:\n",
        "                os.makedirs(path_file)\n",
        "            lora_name = link.replace(\"/\", \"_\").replace(\".\", \"_\")\n",
        "            lora_file_name = f\"lora_{lora_name}.safetensors\"\n",
        "            if \"civitai.com\" in link and Token:\n",
        "                if \"&\" in link or \"?\" in link:\n",
        "                    civit_link = f\"{link}&token={Token}\"\n",
        "                else:\n",
        "                    civit_link = f\"{link}?token={Token}\"\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$civit_link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            elif not link.startswith(\"/content\"):\n",
        "                if not os.path.isfile(os.path.join(path_file, lora_file_name)):\n",
        "                    !cd \"$path_file\"; wget -O \"$lora_file_name\" \"$link\"\n",
        "                lora_list.append(lora_file_name)\n",
        "                lora_path.append(path_file)\n",
        "            else:\n",
        "                if link.startswith(\"/content/gdrive/MyDrive\"):\n",
        "                    constructed_gdrive_link = link\n",
        "                else:\n",
        "                    constructed_gdrive_link = f\"/content/gdrive/MyDrive/{link}\"\n",
        "                link_from_gdrive = constructed_gdrive_link.split(\"/\")\n",
        "                lora_path.append(\"/\".join([word for word in link_from_gdrive if \".safetensors\" not in word]))\n",
        "                lora_list.append(link_from_gdrive[-1])\n",
        "        lora_weights = [word for word in lora_list if word.endswith(\".safetensors\")]\n",
        "        lora_names = [word.replace(\".safetensors\", \"\") for word in lora_weights]\n",
        "        for p in range(len(lora_weights)):\n",
        "            try:\n",
        "                pipeline.load_lora_weights(f\"{lora_path[p]}/{lora_weights[p]}\", adapter_name=lora_names[p])\n",
        "            except (ValueError):\n",
        "                print(f\"Skipping {lora_weights[p]}...\")\n",
        "        pipeline.set_adapters(lora_names, adapter_weights=scales)\n",
        "        print(\"LoRAs:\")\n",
        "        for lora in lora_weights:\n",
        "            print(lora)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Logic to handle image(s) for IP-Adapter + display\n",
        "    if IP_Adapter != \"None\":\n",
        "        adapter_image = []\n",
        "        simple_Url = [word for word in re.split(r\"\\s*,\\s*\", IP_Image_Link) if word]\n",
        "        for link in simple_Url:\n",
        "            adapter_image.append(load_image(link))\n",
        "        adapter_display = [element for element in adapter_image]\n",
        "        if len(adapter_image) % 3 == 0:\n",
        "            row = len(adapter_image)/3\n",
        "        else:\n",
        "            row = int(len(adapter_image)/3) + 1\n",
        "            for i in range(3*row - len(adapter_image)):\n",
        "                adapter_display.append(load_image(\"https://huggingface.co/IDK-ab0ut/BFIDIW9W29NFJSKAOAOXDOKERJ29W/resolve/main/placeholder.png\"))\n",
        "        print(\"Image(s) for IP-Adapter:\")\n",
        "        display(make_image_grid([element.resize((1024, 1024)) for element in adapter_display], rows=row, cols=3))\n",
        "        image_embeds = [adapter_image]\n",
        "        pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=IP_Adapter)\n",
        "        pipeline.set_ip_adapter_scale(IP_Adapter_Strength)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Generate\n",
        "    if not controlnets and not active_inpaint: # For Text2Img\n",
        "        image_save = \"[Text-to-Image]\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "            ).images[0]\n",
        "        else:\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                ip_adapter_image=image_embeds,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "            ).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    elif active_inpaint and not controlnets: # For Inpainting\n",
        "        image_save = \"[Inpainting]\"\n",
        "        if IP_Adapter == \"None\":\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                image=inpaint_image,\n",
        "                mask_image=mask_image,\n",
        "                generator=generator,\n",
        "                strength=Inpainting_Strength,\n",
        "            ).images[0]\n",
        "        else:\n",
        "            image = pipeline(\n",
        "                prompt_embeds=conditioning[0:1],\n",
        "                pooled_prompt_embeds=pooled[0:1],\n",
        "                negative_prompt_embeds=conditioning[1:2],\n",
        "                negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                num_inference_steps=Steps,\n",
        "                ip_adapter_image=image_embeds,\n",
        "                width=Width,\n",
        "                height=Height,\n",
        "                guidance_scale=Scale,\n",
        "                clip_skip=Clip_Skip,\n",
        "                generator=generator,\n",
        "                image=inpaint_image,\n",
        "                mask_image=mask_image,\n",
        "                strength=Inpainting_Strength,\n",
        "            ).images[0]\n",
        "            pipeline.unload_ip_adapter()\n",
        "    else: # For ControlNet\n",
        "        image_save = \"[ControlNet]\"\n",
        "        if Inpainting: # Deprecated. Will raise an error if both ControlNet and Inpainting collide.\n",
        "            '''\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                ).images[0]\n",
        "            '''\n",
        "        else:\n",
        "            if IP_Adapter == \"None\":\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    num_inference_steps=Steps,\n",
        "                    generator=generator,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale,\n",
        "                    guidance_scale=Scale,\n",
        "                ).images[0]\n",
        "            else:\n",
        "                image = pipeline(\n",
        "                    prompt_embeds=conditioning[0:1],\n",
        "                    pooled_prompt_embeds=pooled[0:1],\n",
        "                    negative_prompt_embeds=conditioning[1:2],\n",
        "                    negative_pooled_prompt_embeds=pooled[1:2],\n",
        "                    num_inference_steps=Steps,\n",
        "                    ip_adapter_image=image_embeds,\n",
        "                    width=Width,\n",
        "                    height=Height,\n",
        "                    guidance_scale=Scale,\n",
        "                    clip_skip=Clip_Skip,\n",
        "                    generator=generator,\n",
        "                    image=images,\n",
        "                    controlnet_conditioning_scale=controlnets_scale\n",
        "                ).images[0]\n",
        "\n",
        "    # Saving the image\n",
        "    current_time = time.localtime()\n",
        "    formatted_time = time.strftime(\"[%H-%M-%S %B %d, %Y]\", current_time)\n",
        "    if Save_and_Connect_To_GDrive:\n",
        "        if image_save == \"[Text-to-Image]\":\n",
        "            image_save_path = \"/content/gdrive/MyDrive/Text2Img\"\n",
        "        elif image_save == \"[ControlNet]\":\n",
        "            image_save_path = \"/content/gdrive/MyDrive/ControlNet\"\n",
        "        else:\n",
        "            image_save_path = \"/content/gdrive/MyDrive/Inpainting\"\n",
        "    else:\n",
        "        if image_save == \"[Text-to-Image]\":\n",
        "            image_save_path = \"/content/Text2Img\"\n",
        "        elif image_save == \"[ControlNet]\":\n",
        "            image_save_path = \"/content/ControlNet\"\n",
        "        else:\n",
        "            image_save_path = \"/content/Inpainting\"\n",
        "    if not os.path.exists(image_save_path):\n",
        "            os.makedirs(image_save_path)\n",
        "    split_prompt = re.split(\"\\s*,\\s*\", Prompt.replace(\"<\", \"\").replace(\">\", \"\").replace(\":\", \"_\").replace(\";\", \"_\"))\n",
        "    prompt_name = \" \".join(split_prompt)\n",
        "    generated_image_raw_filename = f\"{image_save} {formatted_time} {prompt_name}\"\n",
        "    generated_image_filename = generated_image_raw_filename[:251] if len(generated_image_raw_filename) > 255 else generated_image_raw_filename\n",
        "    generated_image_savefile = f\"{image_save_path}/{generated_image_filename}.png\"\n",
        "    image.save(generated_image_savefile)\n",
        "    widget()\n",
        "\n",
        "    # Saving parameters config 2nd phase\n",
        "    params = [\n",
        "        prompt_widget.value,\n",
        "        model_widget.value,\n",
        "        model_format_widget.value,\n",
        "        negative_prompt_widget.value,\n",
        "        width_slider.value,\n",
        "        height_slider.value,\n",
        "        scheduler_dropdown.value,\n",
        "        steps_slider.value,\n",
        "        scale_slider.value,\n",
        "        vae_link_widget.value,\n",
        "        clip_skip_slider.value,\n",
        "        lora_urls_widget.value,\n",
        "        weight_scale_widget.value,\n",
        "        canny_min_slider.value,\n",
        "        canny_max_slider.value,\n",
        "        canny_link_widget.value,\n",
        "        canny_toggle.value,\n",
        "        canny_strength_slider.value,\n",
        "        depth_map_link_widget.value,\n",
        "        depth_map_toggle.value,\n",
        "        depth_strength_slider.value,\n",
        "        openpose_link_widget.value,\n",
        "        openpose_toggle.value,\n",
        "        openpose_strength_slider.value,\n",
        "        inpainting_image_dropdown.value,\n",
        "        mask_image_widget.value,\n",
        "        inpainting_toggle.value,\n",
        "        inpainting_strength_slider.value,\n",
        "        ip_adapter_dropdown.value,\n",
        "        ip_image_link_widget.value,\n",
        "        ip_adapter_strength_slider.value,\n",
        "        freeze_widget.value,\n",
        "        karras_bool.value,\n",
        "        vpred_bool.value,\n",
        "        sgmuniform_bool.value,\n",
        "        res_betas_zero_snr.value,\n",
        "        vae_config.value\n",
        "    ]\n",
        "    save_param(f\"{base_path}/parameters.json\", params)\n",
        "\n",
        "    # Handling last generated image\n",
        "    last_generation_json = os.path.join(base_path, \"last_generation.json\")\n",
        "    save_last(last_generation_json, generated_image_savefile, image_save)\n",
        "\n",
        "    # Displaying the image, seed, and scheduler\n",
        "    display(image)\n",
        "    print(f\"Scheduler: {''.join(Scheduler_used)}\")\n",
        "    print(f\"Seed: {saved_number}\")\n",
        "    print(f\"Image is saved at {generated_image_savefile}.\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "submit_button_widget.on_click(submit)\n",
        "widget()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###<font color=\"black\"> Â» <b><font color=\"purple\">Information </b>âœï¸ğŸ“„</font> <font color=\"black\"> Â«\n",
        "#####ã…¤\n",
        "<small>â€¢ Text2Img image is saved in Text2Img folder. </small>\n",
        "\n",
        "<small>â€¢ ControlNet-generated image is saved in ControlNet folder. (requires **Canny**, **Depth Map**, and/or **Open Pose** to be checked, as well as the direct link to the reference image) </small>\n",
        "\n",
        "<small>â€¢ Inpainting-generated image is saved in Inpainting folder. (requires **Inpainting** to be checked, as well as inputting the image and the mask image)</small>\n",
        "\n",
        "<small> â€¢ You can't combine Inpainting and ControlNet.</small>\n",
        "\n",
        "<small>â€¢ IP-Adapter doesn't change the image name.</small>\n",
        "\n",
        "<small>â€¢ You can load LoRAs from your Google Drive by inputting their path. As of now, only LoRAs are supported. </small>\n",
        "\n",
        "<small>â€¢ For ControlNet, leave the image link blank to use the last generated Text2Img image as the reference. Input \"inpaint\" to use the last generated Inpainting image. And lastly, input \"controlnet\" to use the last generated ControlNet image. (requires **Canny**, **Depth Map**, **Inpainting**, and/or **Open Pose** to be checked) </small>\n",
        "\n",
        "***\n",
        "\n",
        "###<font color=\"black\"> Â» <b><font color=\"cyan\">Guide </b>ğŸš¶ğŸ»ğŸ“‹</font> <font color=\"black\"> Â«\n",
        "#####ã…¤\n",
        "\n",
        "<small> **Prompt:** Basically, this one tells the AI what do you want to see in the image. Sometimes, you have to be strict with your words to align the image with your imagination.\n",
        "\n",
        "<small> **Model (**checkpoint**):** A saved state during an intense training. This is required to generate the image. The type of model you inputted affects the overall style.\n",
        "\n",
        "<small> **Model Format:** This is pretty self-explanatory. If you want to use .safetensors model, then set it to \"Safe Tensors.\"\n",
        "\n",
        "<small> **Steps:** It's simply how many iterations the AI will do in order to generate the image. More doesn't always better. You can look for references online.\n",
        "\n",
        "<small> **Scale (**Guidance Scale**):** This affects how closely related the image with the prompt. High value can be precise, but low value can add extra uniqueness.\n",
        "\n",
        "<small> **VAE:** Stands for Variational Autoencoder. It basically controls the color of your image.\n",
        "\n",
        "<small> **Clip Skip:** Lets the AI skip set amount of layers during generation.\n",
        "\n",
        "<small> **LoRA:** Stands for Low-Rank Adaptation. It holds weight to be \"fed\" to the AI. In simple terms, LoRA guides the AI to draw specific characters, style, poses, and so much more. You also need to specify the LoRA's scale, similar to **Scale**.\n",
        "\n",
        "<small> **ControlNet:** Basically a strict instruction based on the inputted image to generate image closely related to the reference.\n",
        "\n",
        "<small> **Inpainting:** Redrawing an image, but with certain parts of the image changed, just like editing with Photoshop, but AI does the job for you.\n",
        "\n",
        "<small> **IP-Adapter:** Similar to LoRA, but only follows the inputted image(s). This is stricter than LoRA and sometimes lacks generalization.\n",
        "\n",
        "<small> **Negative Prompt:** The reverse version of **Prompt**. Instead of telling the AI what do you want, this tells the AI about what do you want to be removed from the image."
      ],
      "metadata": {
        "id": "xMmnx4mwtupU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t8hug-0Okf8t"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
